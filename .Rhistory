# Load required libraries
library(jsonlite)
library(httr)
# URL of the JSON data
url <- "https://catalog.data.gov/harvest/object/7333b45b-4b59-49f2-8ca7-fe202f846cc0"
# Make a GET request to fetch the JSON data
response <- GET(url)
status_code(response)
# Parse the JSON content
content <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(content)
json_data
# Extract the 'distribution' section or another relevant part
distribution_data <- json_data$distribution
# Convert the extracted section to a data frame (if not already in one)
distribution_df <- as.data.frame(distribution_data)
# Define the path for the CSV file
csv_file_path <- "alternative_fuel_stations_distribution.csv"
# Save the data frame to a CSV file
write.csv(distribution_df, csv_file_path, row.names = FALSE)
getwd()
library(rvest)
library(httr)
# Define the URL of the webpage to scrape
url <- 'https://catalog.data.gov/organization/doe-gov?q=&sort=views_recent+desc&res_format=CSV'
# Use httr to handle the page request
page <- httr::GET(url)
http_status(page)$category == "success"
# Use httr to handle the page request
page <- httr::GET(url)
# Parse the HTML content of the page
content <- content(page, as = "text")
html <- read_html(content)
html
# Use rvest to find the dataset links
dataset_links <- html %>%
html_nodes('SELECTOR_FOR_DATASET_LINKS') %>%
html_attr('href')
# Print the dataset links
print(dataset_links)
# Use rvest to find the dataset links
dataset_links <- html %>%
html_nodes('dataset-heading') %>%
html_attr('href')
# Print the dataset links
print(dataset_links)
http_status(page)$category
content <- content(page, as = "text")
html <- read_html(content)
# Use rvest to find the dataset links
dataset_links <- page %>%
html_nodes("h3.dataset-heading a") %>%
html_attr("href")
# Use httr to handle the page request
page <- httr::GET(url)
# Parse the HTML content of the page
content <- content(page, as = "text")
html <- read_html(content)
# Use rvest to find the dataset links
dataset_links <- page %>%
html_nodes("h3.dataset-heading a") %>%
html_attr("href")
library(rvest)
library(httr)
# Define the URL of the page to scrape
url <- 'https://catalog.data.gov/organization/doe-gov?q=&sort=views_recent+desc&res_format=CSV'
# First, make sure you're getting the correct response from the URL
response <- httr::GET(url)
response$status_code
http_status(page)$category
# Parse the content as HTML
page <- read_html(response)
page
writeLines(page, "test.htl")
# Use CSS selectors to find the dataset links
dataset_links <- page %>%
html_nodes("h3.dataset-heading a") %>%
html_attr("href")
dataset_links
# Prepend the base URL if the links are relative
base_url <- "https://catalog.data.gov"
dataset_urls <- paste0(base_url, dataset_links)
# Print the dataset URLs
print(dataset_urls)
library(rvest)
library(httr)
# Base URL
base_url <- "https://catalog.data.gov"
# Function to get dataset URLs
get_dataset_urls <- function(page_url) {
page <- read_html(page_url)
links <- page %>%
html_nodes("h3.dataset-heading a") %>%
html_attr("href")
full_links <- paste0(base_url, links)
return(full_links)
}
# Function to get metadata JSON URL
get_metadata_json_url <- function(dataset_url) {
page <- read_html(dataset_url)
json_link <- page %>%
html_nodes("a:contains('Download Metadata')") %>%
html_attr("href")
full_json_link <- paste0(base_url, json_link)
return(full_json_link)
}
# Get all dataset URLs from the main page
dataset_page_url <- 'https://catalog.data.gov/organization/doe-gov?q=&sort=views_recent+desc&res_format=CSV'
dataset_urls <- get_dataset_urls(dataset_page_url)
# Initialize a vector to store JSON file URLs
metadata_json_urls <- c()
# Loop through each dataset URL to find the metadata JSON URL
for (dataset_url in dataset_urls) {
json_url <- get_metadata_json_url(dataset_url)
metadata_json_urls <- c(metadata_json_urls, json_url)
}
# Print the metadata JSON URLs
print(metadata_json_urls)
# wrap the above as a function. Returns a data frame if successful, otherwise NULL
fetch_and_parse_json_data <- function(url) {
response <- GET(url)
if (status_code(response) == 200) {
content <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(content)
distribution_data <- json_data$distribution
distribution_df <- as.data.frame(distribution_data)
return(distribution_df)
} else {
cat("Failed to fetch data. Status code:", status_code(response), "\n")
return(NULL)
}
}
# URL of the JSON data
url <- "https://catalog.data.gov/harvest/object/7333b45b-4b59-49f2-8ca7-fe202f846cc0"
# wrap the above as a function. Returns a data frame if successful, otherwise NULL
fetch_metadata <- function(url) {
response <- GET(url)
if (status_code(response) == 200) {
content <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(content)
distribution_data <- json_data$distribution
distribution_df <- as.data.frame(distribution_data)
return(distribution_df)
} else {
cat("Failed to fetch data. Status code:", status_code(response), "\n")
return(NULL)
}
}
fetch_metadata(url)
shiny::runApp('C:/Users/xijin.ge/Downloads/testapp')
runApp('C:/Users/xijin.ge/Downloads/testapp')
# Set options here
{
options(golem.app.prod = FALSE) # TRUE = production mode, FALSE = development mode
# Detach all loaded packages and clean your environment
golem::detach_all_attached()
# Document and reload your package
golem::document_and_reload()
#  styler::style_pkg()
# Run the application
run_app()
}
shiny::runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
shiny::runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
head(orgInfo)
orgInfo <- dbGetQuery(convert, paste("select distinct * from orgInfo ORDER BY rowid"))
head(orgInfo)
runApp('C:/work/shinygo')
head(orgInfo)
runApp('C:/work/shinygo')
head(orgInfo)
runApp('C:/work/shinygo')
head(orgInfo)
View(orgInfo)
View(orgInfo)
write.csv(orgInfo, "tem.csv")
getwd()
shiny::runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
runApp('C:/work/shinygo')
remotes::install_github("gexijin/idepGolem@v210pfe", upgrade = "never")
idepGolem::run_app()
# Set options here
{
options(golem.app.prod = FALSE) # TRUE = production mode, FALSE = development mode
# Detach all loaded packages and clean your environment
golem::detach_all_attached()
# Document and reload your package
golem::document_and_reload()
#  styler::style_pkg()
# Run the application
run_app()
}
setwd("C:/Users/xijin.ge/Documents/workshop/toxonline")
install.packages(c(
"rstudioapi","languageserver",
"shiny","ggplot2","ggiraph","bslib","dplyr","stringr","scales",
"sf","feather","shinyjs","bsicons","shinycssloaders","tidygeocoder",
"paletteer","leaflet","units","leaflet.extras"
), repos="https://cloud.r-project.org")
